{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de transformaci√≥n, limpieza y procesado de conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = spark.read.csv('../data/games.csv', header=True, sep=',', inferSchema=True)\n",
    "games_red_df = games_df.selectExpr('winner',\n",
    "                                   'gameDuration as duration',\n",
    "                                   'firstBlood',\n",
    "                                   'firstTower',\n",
    "                                   'firstInhibitor',\n",
    "                                   'firstBaron',\n",
    "                                   'firstDragon',\n",
    "                                   'firstRiftHerald',\n",
    "                                   't1_champ1id',\n",
    "                                   't1_champ2id',\n",
    "                                   't1_champ3id',\n",
    "                                   't1_champ4id',\n",
    "                                   't1_champ5id',\n",
    "                                   't1_towerKills',\n",
    "                                   't1_inhibitorKills',\n",
    "                                   't1_baronKills',\n",
    "                                   't1_dragonKills',\n",
    "                                   't1_riftHeraldKills',\n",
    "                                   't2_champ1id',\n",
    "                                   't2_champ2id',\n",
    "                                   't2_champ3id',\n",
    "                                   't2_champ4id',\n",
    "                                   't2_champ5id',\n",
    "                                   't2_towerKills',\n",
    "                                   't2_inhibitorKills',\n",
    "                                   't2_baronKills',\n",
    "                                   't2_dragonKills',\n",
    "                                   't2_riftHeraldKills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t1_champ1id, t1_champ2id, t1_champ3id, t1_champ4id, t1_champ5id), \",\" )' )\n",
    "games_red_cv_df = games_red_df.withColumn('t1_members_str',new_column_expression).drop('t1_champ1id', 't1_champ2id', 't1_champ3id', 't1_champ4id', 't1_champ5id')\n",
    "cv = CountVectorizer(inputCol='t1_members_str', outputCol='t1_members')\n",
    "model = cv.fit(games_red_cv_df)\n",
    "games_red_cv_df=model.transform(games_red_cv_df).drop('t1_members_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t2_champ1id, t2_champ2id, t2_champ3id, t2_champ4id, t2_champ5id), \",\" )' )\n",
    "games_red_cv_df = games_red_cv_df.withColumn('t2_members_str',new_column_expression).drop('t2_champ1id', 't2_champ2id', 't2_champ3id', 't2_champ4id', 't2_champ5id')\n",
    "cv = CountVectorizer(inputCol='t2_members_str', outputCol='t2_members')\n",
    "model = cv.fit(games_red_cv_df)\n",
    "games_red_cv_df=model.transform(games_red_cv_df).drop('t2_members_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.sql.types import DoubleType\n",
    "games_red_cv_df = games_red_cv_df.withColumn('winner', games_red_cv_df.winner.cast(DoubleType()))\n",
    "transformer=Binarizer(inputCol='winner', outputCol='winner_b', threshold=1)\n",
    "games_red_cv_df=transformer.transform(games_red_cv_df).drop('winner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "columns = ['firstBlood', 'firstTower', 'firstInhibitor', 'firstBaron', 'firstDragon', 'firstRiftHerald']\n",
    "new_columns = [ 'b_firstBlood', 'b_firstTower', 'b_firstInhibitor', 'b_firstBaron', 'b_firstDragon', 'b_firstRiftHerald' ]\n",
    "model = OneHotEncoderEstimator(inputCols=columns, outputCols=new_columns,dropLast=False)\n",
    "transformer=model.fit(games_red_cv_df)\n",
    "games_red_cv_df=transformer.transform(games_red_cv_df)\n",
    "for column in columns :\n",
    "    games_red_cv_df = games_red_cv_df.drop(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "columns = [\"duration\", \"t1_towerKills\", \"t1_inhibitorKills\", \"t1_baronKills\", \"t1_dragonKills\", \"t1_riftHeraldKills\",\n",
    "          \"t2_towerKills\", \"t2_inhibitorKills\", \"t2_baronKills\", \"t2_dragonKills\", \"t2_riftHeraldKills\"]\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"assembledColumns\")\n",
    "games_red_cv_df=assembler.transform(games_red_cv_df)\n",
    "model = StandardScaler(inputCol=\"assembledColumns\", outputCol=\"standardColumns\", withStd=True, withMean=True)\n",
    "transformer = model.fit(games_red_cv_df)\n",
    "games_red_cv_df=transformer.transform(games_red_cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['t1_members', 't2_members', 'b_firstBlood', 'b_firstBaron', 'b_firstDragon', 'b_firstInhibitor', 'b_firstRiftHerald', 'b_firstTower', 'standardColumns']\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "games_red_cv_df = assembler.transform(games_red_cv_df)\n",
    "dataset = games_red_cv_df.selectExpr('winner_b as label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "model = PCA(inputCol='features', outputCol='red_features',k=50)\n",
    "transformer = model.fit(dataset)\n",
    "red_dataset=transformer.transform(dataset).drop('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de entrenamiento con Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "[train_df, test_df]=games_red_df.randomSplit([0.7, 0.3])\n",
    "train_df = train_df.withColumn('winner', train_df.winner.cast(DoubleType()))\n",
    "test_df = test_df.withColumn('winner', test_df.winner.cast(DoubleType()))\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t1_champ1id, t1_champ2id, t1_champ3id, t1_champ4id, t1_champ5id), \",\" )' )\n",
    "train_df = train_df.withColumn('t1_members_str',new_column_expression)\n",
    "test_df = test_df.withColumn('t1_members_str', new_column_expression)\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t2_champ1id, t2_champ2id, t2_champ3id, t2_champ4id, t2_champ5id), \",\" )' )\n",
    "train_df = train_df.withColumn('t2_members_str',new_column_expression)\n",
    "test_df = test_df.withColumn('t2_members_str', new_column_expression)\n",
    "cv1 = CountVectorizer(inputCol='t1_members_str', outputCol='t1_members')\n",
    "cv2 = CountVectorizer(inputCol='t2_members_str', outputCol='t2_members')\n",
    "binarizer=Binarizer(inputCol='winner', outputCol='winner_b', threshold=1)\n",
    "columns = ['firstBlood', 'firstTower', 'firstInhibitor', 'firstBaron', 'firstDragon', 'firstRiftHerald']\n",
    "new_columns = [ 'b_firstBlood', 'b_firstTower', 'b_firstInhibitor', 'b_firstBaron', 'b_firstDragon', 'b_firstRiftHerald' ]\n",
    "ohe = OneHotEncoderEstimator(inputCols=columns, outputCols=new_columns,dropLast=False)\n",
    "columns = [\"duration\", \"t1_towerKills\", \"t1_inhibitorKills\", \"t1_baronKills\", \"t1_dragonKills\", \"t1_riftHeraldKills\",\n",
    "          \"t2_towerKills\", \"t2_inhibitorKills\", \"t2_baronKills\", \"t2_dragonKills\", \"t2_riftHeraldKills\"]\n",
    "assembler1 = VectorAssembler(inputCols=columns, outputCol=\"assembledColumns\")\n",
    "scaler = StandardScaler(inputCol=\"assembledColumns\", outputCol=\"standardColumns\", withStd=True, withMean=True)\n",
    "columns = ['t1_members', 't2_members', 'b_firstBlood', 'b_firstBaron', 'b_firstDragon', 'b_firstInhibitor', 'b_firstRiftHerald', 'b_firstTower', 'standardColumns']\n",
    "assembler2 = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "pca = PCA(inputCol='features', outputCol='red_features')\n",
    "logistic = LogisticRegression(labelCol='winner_b',featuresCol='red_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder().addGrid(logistic.elasticNetParam, [0, 0.33]).\\\n",
    "    addGrid(pca.k, [50, 100]).addGrid(logistic.regParam, [0.1, 0.33]).addGrid(logistic.maxIter,[50]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='winner_b')\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, pca, logistic])\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params).setNumFolds(3)\n",
    "model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9609312382568909 50 0.1 0.0\n",
      "0.9600947005642335 50 0.33 0.0\n",
      "0.9615464461090166 100 0.1 0.0\n",
      "0.959758767703472 100 0.33 0.0\n",
      "0.9471420499615943 50 0.1 0.33\n",
      "0.9278745831433264 50 0.33 0.33\n",
      "0.9471420499615943 100 0.1 0.33\n",
      "0.9278745831433264 100 0.33 0.33\n"
     ]
    }
   ],
   "source": [
    "for (result, config) in zip(model.avgMetrics, params) :\n",
    "    print(result, config[pca.k], config[logistic.regParam], config[logistic.elasticNetParam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960321510589436"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
