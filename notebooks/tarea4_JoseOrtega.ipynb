{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de transformación, limpieza y procesado de conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = spark.read.csv('../data/games.csv', header=True, sep=',', inferSchema=True)\n",
    "games_red_df = games_df.selectExpr('winner',\n",
    "                                   'gameDuration as duration',\n",
    "                                   'firstBlood',\n",
    "                                   'firstTower',\n",
    "                                   'firstInhibitor',\n",
    "                                   'firstBaron',\n",
    "                                   'firstDragon',\n",
    "                                   'firstRiftHerald',\n",
    "                                   't1_champ1id',\n",
    "                                   't1_champ2id',\n",
    "                                   't1_champ3id',\n",
    "                                   't1_champ4id',\n",
    "                                   't1_champ5id',\n",
    "                                   't1_towerKills',\n",
    "                                   't1_inhibitorKills',\n",
    "                                   't1_baronKills',\n",
    "                                   't1_dragonKills',\n",
    "                                   't1_riftHeraldKills',\n",
    "                                   't2_champ1id',\n",
    "                                   't2_champ2id',\n",
    "                                   't2_champ3id',\n",
    "                                   't2_champ4id',\n",
    "                                   't2_champ5id',\n",
    "                                   't2_towerKills',\n",
    "                                   't2_inhibitorKills',\n",
    "                                   't2_baronKills',\n",
    "                                   't2_dragonKills',\n",
    "                                   't2_riftHeraldKills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t1_champ1id, t1_champ2id, t1_champ3id, t1_champ4id, t1_champ5id), \",\" )' )\n",
    "games_red_cv_df = games_red_df.withColumn('t1_members_str',new_column_expression).drop('t1_champ1id', 't1_champ2id', 't1_champ3id', 't1_champ4id', 't1_champ5id')\n",
    "cv = CountVectorizer(inputCol='t1_members_str', outputCol='t1_members')\n",
    "model = cv.fit(games_red_cv_df)\n",
    "games_red_cv_df=model.transform(games_red_cv_df).drop('t1_members_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t2_champ1id, t2_champ2id, t2_champ3id, t2_champ4id, t2_champ5id), \",\" )' )\n",
    "games_red_cv_df = games_red_cv_df.withColumn('t2_members_str',new_column_expression).drop('t2_champ1id', 't2_champ2id', 't2_champ3id', 't2_champ4id', 't2_champ5id')\n",
    "cv = CountVectorizer(inputCol='t2_members_str', outputCol='t2_members')\n",
    "model = cv.fit(games_red_cv_df)\n",
    "games_red_cv_df=model.transform(games_red_cv_df).drop('t2_members_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.sql.types import DoubleType\n",
    "games_red_cv_df = games_red_cv_df.withColumn('winner', games_red_cv_df.winner.cast(DoubleType()))\n",
    "transformer=Binarizer(inputCol='winner', outputCol='winner_b', threshold=1)\n",
    "games_red_cv_df=transformer.transform(games_red_cv_df).drop('winner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "columns = ['firstBlood', 'firstTower', 'firstInhibitor', 'firstBaron', 'firstDragon', 'firstRiftHerald']\n",
    "new_columns = [ 'b_firstBlood', 'b_firstTower', 'b_firstInhibitor', 'b_firstBaron', 'b_firstDragon', 'b_firstRiftHerald' ]\n",
    "model = OneHotEncoderEstimator(inputCols=columns, outputCols=new_columns,dropLast=False)\n",
    "transformer=model.fit(games_red_cv_df)\n",
    "games_red_cv_df=transformer.transform(games_red_cv_df)\n",
    "for column in columns :\n",
    "    games_red_cv_df = games_red_cv_df.drop(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "columns = [\"duration\", \"t1_towerKills\", \"t1_inhibitorKills\", \"t1_baronKills\", \"t1_dragonKills\", \"t1_riftHeraldKills\",\n",
    "          \"t2_towerKills\", \"t2_inhibitorKills\", \"t2_baronKills\", \"t2_dragonKills\", \"t2_riftHeraldKills\"]\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"assembledColumns\")\n",
    "games_red_cv_df=assembler.transform(games_red_cv_df)\n",
    "model = StandardScaler(inputCol=\"assembledColumns\", outputCol=\"standardColumns\", withStd=True, withMean=True)\n",
    "transformer = model.fit(games_red_cv_df)\n",
    "games_red_cv_df=transformer.transform(games_red_cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['t1_members', 't2_members', 'b_firstBlood', 'b_firstBaron', 'b_firstDragon', 'b_firstInhibitor', 'b_firstRiftHerald', 'b_firstTower', 'standardColumns']\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "games_red_cv_df = assembler.transform(games_red_cv_df)\n",
    "dataset = games_red_cv_df.selectExpr('winner_b as label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "model = PCA(inputCol='features', outputCol='red_features',k=50)\n",
    "transformer = model.fit(dataset)\n",
    "red_dataset=transformer.transform(dataset).drop('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de entrenamiento con Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "[train_df, test_df]=games_red_df.randomSplit([0.7, 0.3])\n",
    "train_df = train_df.withColumn('winner', train_df.winner.cast(DoubleType()))\n",
    "test_df = test_df.withColumn('winner', test_df.winner.cast(DoubleType()))\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t1_champ1id, t1_champ2id, t1_champ3id, t1_champ4id, t1_champ5id), \",\" )' )\n",
    "train_df = train_df.withColumn('t1_members_str',new_column_expression)\n",
    "test_df = test_df.withColumn('t1_members_str', new_column_expression)\n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t2_champ1id, t2_champ2id, t2_champ3id, t2_champ4id, t2_champ5id), \",\" )' )\n",
    "train_df = train_df.withColumn('t2_members_str',new_column_expression)\n",
    "test_df = test_df.withColumn('t2_members_str', new_column_expression)\n",
    "cv1 = CountVectorizer(inputCol='t1_members_str', outputCol='t1_members')\n",
    "cv2 = CountVectorizer(inputCol='t2_members_str', outputCol='t2_members')\n",
    "binarizer=Binarizer(inputCol='winner', outputCol='winner_b', threshold=1)\n",
    "columns = ['firstBlood', 'firstTower', 'firstInhibitor', 'firstBaron', 'firstDragon', 'firstRiftHerald']\n",
    "new_columns = [ 'b_firstBlood', 'b_firstTower', 'b_firstInhibitor', 'b_firstBaron', 'b_firstDragon', 'b_firstRiftHerald' ]\n",
    "ohe = OneHotEncoderEstimator(inputCols=columns, outputCols=new_columns,dropLast=False)\n",
    "columns = [\"duration\", \"t1_towerKills\", \"t1_inhibitorKills\", \"t1_baronKills\", \"t1_dragonKills\", \"t1_riftHeraldKills\",\n",
    "          \"t2_towerKills\", \"t2_inhibitorKills\", \"t2_baronKills\", \"t2_dragonKills\", \"t2_riftHeraldKills\"]\n",
    "assembler1 = VectorAssembler(inputCols=columns, outputCol=\"assembledColumns\")\n",
    "scaler = StandardScaler(inputCol=\"assembledColumns\", outputCol=\"standardColumns\", withStd=True, withMean=True)\n",
    "columns = ['t1_members', 't2_members', 'b_firstBlood', 'b_firstBaron', 'b_firstDragon', 'b_firstInhibitor', 'b_firstRiftHerald', 'b_firstTower', 'standardColumns']\n",
    "assembler2 = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "pca = PCA(inputCol='features', outputCol='red_features')\n",
    "logistic = LogisticRegression(labelCol='winner_b',featuresCol='red_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9601451366940987 50 0.1 0.0\n",
      "0.867753558224236 50 0.33 0.0\n",
      "0.9600618424306799 100 0.1 0.0\n",
      "0.9587861216949627 100 0.33 0.0\n",
      "0.8592288152762125 50 0.1 0.33\n",
      "0.8433579171399568 50 0.33 0.33\n",
      "0.9482952576647744 100 0.1 0.33\n",
      "0.8433579171399568 100 0.33 0.33\n",
      "Tiempo de ejecucion-----> 0:01:28.729545\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "\n",
    "params = ParamGridBuilder().addGrid(logistic.elasticNetParam, [0, 0.33]).\\\n",
    "    addGrid(pca.k, [50, 100]).addGrid(logistic.regParam, [0.1, 0.33]).build()\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='winner_b')\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, pca, logistic])\n",
    "#Cambiamos el numero del folds a 5\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params).setNumFolds(5)\n",
    "#Creamos una variable que guarda la hora en la que empieza la ejecución\n",
    "now=datetime.datetime.now()\n",
    "model = cv.fit(train_df)\n",
    "#Guardamos otra variable con la hora a la que acaba\n",
    "after=datetime.datetime.now()\n",
    "for (result, config) in zip(model.avgMetrics, params) :\n",
    "    print(result, config[pca.k], config[logistic.regParam], config[logistic.elasticNetParam])\n",
    "evaluator.evaluate(model.transform(test_df))\n",
    "#Mostramos la diferencia de tiemnpo entre que empezo y acabo\n",
    "print('Tiempo de ejecucion----->',after-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9600593200732261 50 0.1 0.0\n",
      "0.9600593200732261 50 0.1 0.0\n",
      "0.9600593200732261 50 0.1 0.0\n",
      "0.9589229406334621 50 0.33 0.0\n",
      "0.9589229406334621 50 0.33 0.0\n",
      "0.9589229406334621 50 0.33 0.0\n",
      "0.9578712505151477 50 1.0 0.0\n",
      "0.9578712505151477 50 1.0 0.0\n",
      "0.9578712505151477 50 1.0 0.0\n",
      "0.9423054288385706 50 5.0 0.0\n",
      "0.9423054288385706 50 5.0 0.0\n",
      "0.9423054288385706 50 5.0 0.0\n",
      "0.9105710962821523 50 9.0 0.0\n",
      "0.9105710962821523 50 9.0 0.0\n",
      "0.9105710962821523 50 9.0 0.0\n",
      "0.9595047744156031 100 0.1 0.0\n",
      "0.9595047744156031 100 0.1 0.0\n",
      "0.9595047744156031 100 0.1 0.0\n",
      "0.958421654712782 100 0.33 0.0\n",
      "0.958421654712782 100 0.33 0.0\n",
      "0.958421654712782 100 0.33 0.0\n",
      "0.9581732884458649 100 1.0 0.0\n",
      "0.9581732884458649 100 1.0 0.0\n",
      "0.9581732884458649 100 1.0 0.0\n",
      "0.9436092729568658 100 5.0 0.0\n",
      "0.9436092729568658 100 5.0 0.0\n",
      "0.9436092729568658 100 5.0 0.0\n",
      "0.9133710893127598 100 9.0 0.0\n",
      "0.9133710893127598 100 9.0 0.0\n",
      "0.9133710893127598 100 9.0 0.0\n",
      "0.9480431586926086 50 0.1 0.33\n",
      "0.9480431586926086 50 0.1 0.33\n",
      "0.9480431586926086 50 0.1 0.33\n",
      "0.9274771088504841 50 0.33 0.33\n",
      "0.9274771088504841 50 0.33 0.33\n",
      "0.9274771088504841 50 0.33 0.33\n",
      "0.9129088881537102 50 1.0 0.33\n",
      "0.9129088881537102 50 1.0 0.33\n",
      "0.9129088881537102 50 1.0 0.33\n",
      "0.5051597249361146 50 5.0 0.33\n",
      "0.5051597249361146 50 5.0 0.33\n",
      "0.5051597249361146 50 5.0 0.33\n",
      "0.5051597249361146 50 9.0 0.33\n",
      "0.5051597249361146 50 9.0 0.33\n",
      "0.5051597249361146 50 9.0 0.33\n",
      "0.9480431586926086 100 0.1 0.33\n",
      "0.9480431586926086 100 0.1 0.33\n",
      "0.9480431586926086 100 0.1 0.33\n",
      "0.9274771088504841 100 0.33 0.33\n",
      "0.9274771088504841 100 0.33 0.33\n",
      "0.9274771088504841 100 0.33 0.33\n",
      "0.9129088881537102 100 1.0 0.33\n",
      "0.9129088881537102 100 1.0 0.33\n",
      "0.9129088881537102 100 1.0 0.33\n",
      "0.5051597249361146 100 5.0 0.33\n",
      "0.5051597249361146 100 5.0 0.33\n",
      "0.5051597249361146 100 5.0 0.33\n",
      "0.5051597249361146 100 9.0 0.33\n",
      "0.5051597249361146 100 9.0 0.33\n",
      "0.5051597249361146 100 9.0 0.33\n",
      "0.9372183186738603 50 0.1 0.66\n",
      "0.9372183186738603 50 0.1 0.66\n",
      "0.9372183186738603 50 0.1 0.66\n",
      "0.9274222308462523 50 0.33 0.66\n",
      "0.9274222308462523 50 0.33 0.66\n",
      "0.9274222308462523 50 0.33 0.66\n",
      "0.5051597249361146 50 1.0 0.66\n",
      "0.5051597249361146 50 1.0 0.66\n",
      "0.5051597249361146 50 1.0 0.66\n",
      "0.5051597249361146 50 5.0 0.66\n",
      "0.5051597249361146 50 5.0 0.66\n",
      "0.5051597249361146 50 5.0 0.66\n",
      "0.5051597249361146 50 9.0 0.66\n",
      "0.5051597249361146 50 9.0 0.66\n",
      "0.5051597249361146 50 9.0 0.66\n",
      "0.9372183186738603 100 0.1 0.66\n",
      "0.9372183186738603 100 0.1 0.66\n",
      "0.9372183186738603 100 0.1 0.66\n",
      "0.9274222308462523 100 0.33 0.66\n",
      "0.9274222308462523 100 0.33 0.66\n",
      "0.9274222308462523 100 0.33 0.66\n",
      "0.5051597249361146 100 1.0 0.66\n",
      "0.5051597249361146 100 1.0 0.66\n",
      "0.5051597249361146 100 1.0 0.66\n",
      "0.5051597249361146 100 5.0 0.66\n",
      "0.5051597249361146 100 5.0 0.66\n",
      "0.5051597249361146 100 5.0 0.66\n",
      "0.5051597249361146 100 9.0 0.66\n",
      "0.5051597249361146 100 9.0 0.66\n",
      "0.5051597249361146 100 9.0 0.66\n",
      "0.92767178041175 50 0.1 1.0\n",
      "0.92767178041175 50 0.1 1.0\n",
      "0.92767178041175 50 0.1 1.0\n",
      "0.9270068175456025 50 0.33 1.0\n",
      "0.9270068175456025 50 0.33 1.0\n",
      "0.9270068175456025 50 0.33 1.0\n",
      "0.5051597249361146 50 1.0 1.0\n",
      "0.5051597249361146 50 1.0 1.0\n",
      "0.5051597249361146 50 1.0 1.0\n",
      "0.5051597249361146 50 5.0 1.0\n",
      "0.5051597249361146 50 5.0 1.0\n",
      "0.5051597249361146 50 5.0 1.0\n",
      "0.5051597249361146 50 9.0 1.0\n",
      "0.5051597249361146 50 9.0 1.0\n",
      "0.5051597249361146 50 9.0 1.0\n",
      "0.92767178041175 100 0.1 1.0\n",
      "0.92767178041175 100 0.1 1.0\n",
      "0.92767178041175 100 0.1 1.0\n",
      "0.9270068175456025 100 0.33 1.0\n",
      "0.9270068175456025 100 0.33 1.0\n",
      "0.9270068175456025 100 0.33 1.0\n",
      "0.5051597249361146 100 1.0 1.0\n",
      "0.5051597249361146 100 1.0 1.0\n",
      "0.5051597249361146 100 1.0 1.0\n",
      "0.5051597249361146 100 5.0 1.0\n",
      "0.5051597249361146 100 5.0 1.0\n",
      "0.5051597249361146 100 5.0 1.0\n",
      "0.5051597249361146 100 9.0 1.0\n",
      "0.5051597249361146 100 9.0 1.0\n",
      "0.5051597249361146 100 9.0 1.0\n",
      "Tiempo de ejecucion-----> 0:10:01.695965\n"
     ]
    }
   ],
   "source": [
    "#Hacemos lo mismo pero cambiamos los parametros pedidos\n",
    "params = ParamGridBuilder().addGrid(logistic.elasticNetParam, [0, 0.33,0.66,1]).\\\n",
    "    addGrid(pca.k, [50, 100]).addGrid(logistic.regParam, [0.1, 0.33,1,5,9]).addGrid(logistic.maxIter,[30,50,80]).build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='winner_b')\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, pca, logistic])\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params).setNumFolds(3)\n",
    "now=datetime.datetime.now()\n",
    "model = cv.fit(train_df)\n",
    "after=datetime.datetime.now()\n",
    "for (result, config) in zip(model.avgMetrics, params) :\n",
    "    print(result, config[pca.k], config[logistic.regParam], config[logistic.elasticNetParam])\n",
    "evaluator.evaluate(model.transform(test_df))\n",
    "print('Tiempo de ejecucion----->',after-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929752356104805 50 5 2\n",
      "0.9363003608001947 50 5 5\n",
      "0.943877454259827 50 5 8\n",
      "0.9370229800245051 100 5 2\n",
      "0.9358810495948189 100 5 5\n",
      "0.9377393172529309 100 5 8\n",
      "0.9293899936336554 50 20 2\n",
      "0.9358604957588034 50 20 5\n",
      "0.9474313453917418 50 20 8\n",
      "0.9306932519056448 100 20 2\n",
      "0.9364962158301287 100 20 5\n",
      "0.9409948076497694 100 20 8\n",
      "0.9306092527141347 50 70 2\n",
      "0.9379646414791272 50 70 5\n",
      "0.9473176798725833 50 70 8\n",
      "0.931886347960254 100 70 2\n",
      "0.9379943426246831 100 70 5\n",
      "0.9438843913976569 100 70 8\n",
      "Tiempo de ejecucion-----> 0:03:05.715760\n"
     ]
    }
   ],
   "source": [
    "#LLamamos las librerias que necesitamos\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#Llamamos al arbol de regresion\n",
    "rf = RandomForestClassifier(labelCol=\"winner_b\", featuresCol=\"red_features\")\n",
    "#Tocamos los parametros y procedemos de la misma manera que antes\n",
    "params = ParamGridBuilder().addGrid(rf.numTrees, [5,20,70]).\\\n",
    "    addGrid(pca.k, [50, 100]).addGrid(rf.maxDepth, [2, 5,8]).build()\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"winner_b\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, pca, rf])\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params).setNumFolds(3)\n",
    "now=datetime.datetime.now()\n",
    "model = cv.fit(train_df)\n",
    "after=datetime.datetime.now()\n",
    "for (result, config) in zip(model.avgMetrics, params) :\n",
    "    print(result, config[pca.k], config[rf.numTrees], config[rf.maxDepth])\n",
    "evaluator.evaluate(model.transform(test_df))\n",
    "print('Tiempo de ejecucion----->',after-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929752356104805 50 5 2\n",
      "0.9363003608001947 50 5 5\n",
      "0.94393271509095 50 5 8\n",
      "0.9370229800245051 100 5 2\n",
      "0.9358810495948189 100 5 5\n",
      "0.9376280763607789 100 5 8\n",
      "0.9293899936336554 50 20 2\n",
      "0.9358604957588034 50 20 5\n",
      "0.9467082795927542 50 20 8\n",
      "0.9306932519056448 100 20 2\n",
      "0.9365796464992426 100 20 5\n",
      "0.9414632660619782 100 20 8\n",
      "0.9305539918830119 50 70 2\n",
      "0.9379646414791272 50 70 5\n",
      "0.9473176798725833 50 70 8\n",
      "0.931886347960254 100 70 2\n",
      "0.9376904080535071 100 70 5\n",
      "0.9438843913976569 100 70 8\n",
      "Tiempo de ejecucion-----> 0:03:07.951911\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9651919556670581 30 3\n",
      "0.9638086948440654 30 9\n",
      "0.9677759633645426 70 3\n",
      "0.9650017735747647 70 9\n",
      "Tiempo de ejecucion-----> 0:07:58.075424\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#Llamamos al modelo que vamos a usar. Cambiamos el nombre de la columna features porque quitaremos PCA\n",
    "GBT = GBTClassifier(labelCol=\"winner_b\", featuresCol=\"features\")\n",
    "#Usamos los parametros pedidos y quitamos PCA. Procedemos igual que en los casos anteriores\n",
    "params = ParamGridBuilder().addGrid(GBT.maxIter, [30,70]).\\\n",
    "    addGrid(GBT.maxDepth, [3,9]).build()\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"winner_b\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, GBT])\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params).setNumFolds(3)\n",
    "now=datetime.datetime.now()\n",
    "model = cv.fit(train_df)\n",
    "after=datetime.datetime.now()\n",
    "for (result, config) in zip(model.avgMetrics, params) :\n",
    "    print(result, config[GBT.maxIter], config[GBT.maxDepth])\n",
    "evaluator.evaluate(model.transform(test_df))\n",
    "print('Tiempo de ejecucion----->',after-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630370274803252"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:10\n"
     ]
    }
   ],
   "source": [
    "from datetime \n",
    "delta = timedelta(seconds=10)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIEMPOS MEDIOS\n",
    "#Regresion logistica 1: 11s\n",
    "#Regresion logistica 2: 5s\n",
    "#Árbol: 10s\n",
    "#GBT: 25s\n",
    "#La configuracion más extosa es la del modelo GBT con 70 iteraciones maximas y 3 de profundidad\n",
    "#No parece merecer mucho la pena invertir casi el doble de tiempo para obtener solo unas decimas más de precision\n",
    "#de lo que se obtiene con regresiones como la logistica que son ucho más rapidas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
