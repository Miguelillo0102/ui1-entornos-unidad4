{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basándonos en el ejemplo de validación cruzada empleando Pipelines perteneciente a la sección 4.4.5 de la unidad didáctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos\n",
    "games_df = spark.read.csv('../data/games.csv', header=True, sep=',', inferSchema=True)\n",
    "games_red_df = games_df.selectExpr('winner',\n",
    "                                   'gameDuration as duration',\n",
    "                                   'firstBlood',\n",
    "                                   'firstTower',\n",
    "                                   'firstInhibitor',\n",
    "                                   'firstBaron',\n",
    "                                   'firstDragon',\n",
    "                                   'firstRiftHerald',\n",
    "                                   't1_champ1id',\n",
    "                                   't1_champ2id',\n",
    "                                   't1_champ3id',\n",
    "                                   't1_champ4id',\n",
    "                                   't1_champ5id',\n",
    "                                   't1_towerKills',\n",
    "                                   't1_inhibitorKills',\n",
    "                                   't1_baronKills',\n",
    "                                   't1_dragonKills',\n",
    "                                   't1_riftHeraldKills',\n",
    "                                   't2_champ1id',\n",
    "                                   't2_champ2id',\n",
    "                                   't2_champ3id',\n",
    "                                   't2_champ4id',\n",
    "                                   't2_champ5id',\n",
    "                                   't2_towerKills',\n",
    "                                   't2_inhibitorKills',\n",
    "                                   't2_baronKills',\n",
    "                                   't2_dragonKills',\n",
    "                                   't2_riftHeraldKills')\n",
    "## Reducción de datos para acelerar los procesos. //TODO: Desactivar esto en la ejecución final\n",
    "#print(\"Antes: games_df:\", games_df.count(), \"games_red_df:\", games_red_df.count())\n",
    "#games_df = games_df.sample(fraction = 0.1)\n",
    "#games_red_df = games_red_df.sample(fraction = 0.1)\n",
    "#print(\"Despues: games_df:\", games_df.count(), \"games_red_df:\", games_red_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (1 punto) Emplea validación cruzada con 5 bloques. Añade en este y en los siguientes bloques de entrenamiento, el tiempo que dura cada ejecución de entrenamiento, para ello ayúdate del método now de la librería datetime de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución (hh:mm:ss.ms): 0:02:26.847367\n",
      "0.9603631507638754 50 0.1 0.0\n",
      "0.9596430077838859 50 0.33 0.0\n",
      "0.9604198923050231 100 0.1 0.0\n",
      "0.959419986743417 100 0.33 0.0\n",
      "0.9471384860946084 50 0.1 0.33\n",
      "0.9271863375519438 50 0.33 0.33\n",
      "0.9471384860946084 100 0.1 0.33\n",
      "0.9271863375519438 100 0.33 0.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9617070654976793"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "from pyspark.ml.feature import CountVectorizer, Binarizer, StandardScaler, VectorAssembler, OneHotEncoderEstimator, PCA\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "t_inicial_logistica = dt.now()\n",
    "\n",
    "[train_df, test_df]=games_red_df.randomSplit([0.7, 0.3]) \n",
    "train_df = train_df.withColumn('winner', train_df.winner.cast(DoubleType())) \n",
    "test_df = test_df.withColumn('winner', test_df.winner.cast(DoubleType())) \n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t1_champ1id, t1_champ2id, t1_champ3id, t1_champ4id, t1_champ5id), \",\" )' ) \n",
    "train_df = train_df.withColumn('t1_members_str',new_column_expression) \n",
    "test_df = test_df.withColumn('t1_members_str', new_column_expression) \n",
    "new_column_expression = expr('split( concat_ws( \",\" ,t2_champ1id, t2_champ2id, t2_champ3id, t2_champ4id, t2_champ5id), \",\" )' ) \n",
    "train_df = train_df.withColumn('t2_members_str',new_column_expression) \n",
    "test_df = test_df.withColumn('t2_members_str', new_column_expression) \n",
    "cv1 = CountVectorizer(inputCol='t1_members_str', outputCol='t1_members') \n",
    "cv2 = CountVectorizer(inputCol='t2_members_str', outputCol='t2_members') \n",
    "binarizer=Binarizer(inputCol='winner', outputCol='winner_b', threshold=1) \n",
    "columns = ['firstBlood', 'firstTower', 'firstInhibitor', 'firstBaron', 'firstDragon', 'firstRiftHerald'] \n",
    "new_columns = [ 'b_firstBlood', 'b_firstTower', 'b_firstInhibitor', 'b_firstBaron', 'b_firstDragon', 'b_firstRiftHerald' ] \n",
    "ohe = OneHotEncoderEstimator(inputCols=columns, outputCols=new_columns,dropLast=False) \n",
    "columns = [\"duration\", \"t1_towerKills\", \"t1_inhibitorKills\", \"t1_baronKills\", \"t1_dragonKills\", \"t1_riftHeraldKills\", \"t2_towerKills\", \"t2_inhibitorKills\", \"t2_baronKills\", \"t2_dragonKills\", \"t2_riftHeraldKills\"] \n",
    "assembler1 = VectorAssembler(inputCols=columns, outputCol=\"assembledColumns\")\n",
    "scaler = StandardScaler(inputCol=\"assembledColumns\", outputCol=\"standardColumns\", withStd=True, withMean=True)\n",
    "columns = ['t1_members', 't2_members', 'b_firstBlood', 'b_firstBaron', 'b_firstDragon', 'b_firstInhibitor', 'b_firstRiftHerald', 'b_firstTower', 'standardColumns'] \n",
    "assembler2 = VectorAssembler(inputCols=columns, outputCol='features') \n",
    "pca = PCA(inputCol='features', outputCol='red_features') \n",
    "logistic = LogisticRegression(labelCol='winner_b',featuresCol='red_features')\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder \n",
    "params_logistica = ParamGridBuilder().addGrid(logistic.elasticNetParam, [0, 0.33]).\\\n",
    "addGrid(pca.k, [50, 100]).addGrid(logistic.regParam, [0.1, 0.33]).addGrid(logistic.maxIter,[50]).build()\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='winner_b')\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, pca, logistic]) \n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params_logistica).setNumFolds(5)\n",
    "model_logistica = cv.fit(train_df)\n",
    "\n",
    "t_final_logistica = dt.now()\n",
    "t_ejecucion_logistica = t_final_logistica - t_inicial_logistica\n",
    "print(f\"Tiempo de ejecución (hh:mm:ss.ms): {t_ejecucion_logistica}\")\n",
    "for (result, config) in zip(model_logistica.avgMetrics, params_logistica) :\n",
    "    print(result, config[pca.k], config[logistic.regParam], config[logistic.elasticNetParam])\n",
    "\n",
    "ev_logistica = evaluator.evaluate(model_logistica.transform(test_df))\n",
    "ev_logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (2 puntos) Amplía la horquilla de valores probados para la regresión Logística:\n",
    "\n",
    "    a. elasticNetParam, probando 2 valores más entre 0 y 1.\n",
    "\n",
    "    b. regParam, probando 3 valores más entre 0 y 10.\n",
    "\n",
    "    c. maxIter, probando 2 valores más entre 1 y 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución (hh:mm:ss.ms): 0:21:42.198405\n",
      "0.9603631507638754 50 0.1 0.0 20\n",
      "0.9603631507638754 50 0.1 0.0 50\n",
      "0.9603631507638754 50 0.1 0.0 80\n",
      "0.9596430077838859 50 0.33 0.0 20\n",
      "0.9596430077838859 50 0.33 0.0 50\n",
      "0.9596430077838859 50 0.33 0.0 80\n",
      "0.948004990199687 50 3.0 0.0 20\n",
      "0.948004990199687 50 3.0 0.0 50\n",
      "0.948004990199687 50 3.0 0.0 80\n",
      "0.9280463297632416 50 5.0 0.0 20\n",
      "0.9280463297632416 50 5.0 0.0 50\n",
      "0.9280463297632416 50 5.0 0.0 80\n",
      "0.9604198923050231 100 0.1 0.0 20\n",
      "0.9604198923050231 100 0.1 0.0 50\n",
      "0.9604198923050231 100 0.1 0.0 80\n",
      "0.959419986743417 100 0.33 0.0 20\n",
      "0.959419986743417 100 0.33 0.0 50\n",
      "0.959419986743417 100 0.33 0.0 80\n",
      "0.9487026765292466 100 3.0 0.0 20\n",
      "0.9487026765292466 100 3.0 0.0 50\n",
      "0.9487026765292466 100 3.0 0.0 80\n",
      "0.9301606807772531 100 5.0 0.0 20\n",
      "0.9301606807772531 100 5.0 0.0 50\n",
      "0.9301606807772531 100 5.0 0.0 80\n",
      "0.9471384860946084 50 0.1 0.33 20\n",
      "0.9471384860946084 50 0.1 0.33 50\n",
      "0.9471384860946084 50 0.1 0.33 80\n",
      "0.9271863375519438 50 0.33 0.33 20\n",
      "0.9271863375519438 50 0.33 0.33 50\n",
      "0.9271863375519438 50 0.33 0.33 80\n",
      "0.5076320670360772 50 3.0 0.33 20\n",
      "0.5076320670360772 50 3.0 0.33 50\n",
      "0.5076320670360772 50 3.0 0.33 80\n",
      "0.5076320670360772 50 5.0 0.33 20\n",
      "0.5076320670360772 50 5.0 0.33 50\n",
      "0.5076320670360772 50 5.0 0.33 80\n",
      "0.9471384860946084 100 0.1 0.33 20\n",
      "0.9471384860946084 100 0.1 0.33 50\n",
      "0.9471384860946084 100 0.1 0.33 80\n",
      "0.9271863375519438 100 0.33 0.33 20\n",
      "0.9271863375519438 100 0.33 0.33 50\n",
      "0.9271863375519438 100 0.33 0.33 80\n",
      "0.5076320670360772 100 3.0 0.33 20\n",
      "0.5076320670360772 100 3.0 0.33 50\n",
      "0.5076320670360772 100 3.0 0.33 80\n",
      "0.5076320670360772 100 5.0 0.33 20\n",
      "0.5076320670360772 100 5.0 0.33 50\n",
      "0.5076320670360772 100 5.0 0.33 80\n",
      "0.9424139814629342 50 0.1 0.5 20\n",
      "0.9424139814629342 50 0.1 0.5 50\n",
      "0.9424139814629342 50 0.1 0.5 80\n",
      "0.9273531473767529 50 0.33 0.5 20\n",
      "0.9273531473767529 50 0.33 0.5 50\n",
      "0.9273531473767529 50 0.33 0.5 80\n",
      "0.5076320670360772 50 3.0 0.5 20\n",
      "0.5076320670360772 50 3.0 0.5 50\n",
      "0.5076320670360772 50 3.0 0.5 80\n",
      "0.5076320670360772 50 5.0 0.5 20\n",
      "0.5076320670360772 50 5.0 0.5 50\n",
      "0.5076320670360772 50 5.0 0.5 80\n",
      "0.9424139814629342 100 0.1 0.5 20\n",
      "0.9424139814629342 100 0.1 0.5 50\n",
      "0.9424139814629342 100 0.1 0.5 80\n",
      "0.9273531473767529 100 0.33 0.5 20\n",
      "0.9273531473767529 100 0.33 0.5 50\n",
      "0.9273531473767529 100 0.33 0.5 80\n",
      "0.5076320670360772 100 3.0 0.5 20\n",
      "0.5076320670360772 100 3.0 0.5 50\n",
      "0.5076320670360772 100 3.0 0.5 80\n",
      "0.5076320670360772 100 5.0 0.5 20\n",
      "0.5076320670360772 100 5.0 0.5 50\n",
      "0.5076320670360772 100 5.0 0.5 80\n",
      "0.9323821108597178 50 0.1 0.75 20\n",
      "0.9323821108597178 50 0.1 0.75 50\n",
      "0.9323821108597178 50 0.1 0.75 80\n",
      "0.9271013244096156 50 0.33 0.75 20\n",
      "0.9271013244096156 50 0.33 0.75 50\n",
      "0.9271013244096156 50 0.33 0.75 80\n",
      "0.5076320670360772 50 3.0 0.75 20\n",
      "0.5076320670360772 50 3.0 0.75 50\n",
      "0.5076320670360772 50 3.0 0.75 80\n",
      "0.5076320670360772 50 5.0 0.75 20\n",
      "0.5076320670360772 50 5.0 0.75 50\n",
      "0.5076320670360772 50 5.0 0.75 80\n",
      "0.9323821108597178 100 0.1 0.75 20\n",
      "0.9323821108597178 100 0.1 0.75 50\n",
      "0.9323821108597178 100 0.1 0.75 80\n",
      "0.9271013244096156 100 0.33 0.75 20\n",
      "0.9271013244096156 100 0.33 0.75 50\n",
      "0.9271013244096156 100 0.33 0.75 80\n",
      "0.5076320670360772 100 3.0 0.75 20\n",
      "0.5076320670360772 100 3.0 0.75 50\n",
      "0.5076320670360772 100 3.0 0.75 80\n",
      "0.5076320670360772 100 5.0 0.75 20\n",
      "0.5076320670360772 100 5.0 0.75 50\n",
      "0.5076320670360772 100 5.0 0.75 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9617070654976793"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_inicial_logistica2 = dt.now()\n",
    "params_logistica2 = ParamGridBuilder().addGrid(logistic.elasticNetParam, [0, 0.33, 0.5, 0.75]).\\\n",
    "addGrid(pca.k, [50, 100]).addGrid(logistic.regParam, [0.1, 0.33, 3, 5]).addGrid(logistic.maxIter,[20,50,80]).build()\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params_logistica2).setNumFolds(5)\n",
    "model_logistica2 = cv.fit(train_df)\n",
    "\n",
    "t_final_logistica2 = dt.now()\n",
    "t_ejecucion_logistica2 = t_final_logistica2 - t_inicial_logistica2\n",
    "print(f\"Tiempo de ejecución (hh:mm:ss.ms): {t_ejecucion_logistica2}\")\n",
    "for (result, config) in zip(model_logistica2.avgMetrics, params_logistica2) :\n",
    "    print(result, config[pca.k], config[logistic.regParam], config[logistic.elasticNetParam], config[logistic.maxIter])\n",
    "\n",
    "ev_logistica2 = evaluator.evaluate(model_logistica2.transform(test_df))\n",
    "ev_logistica2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (2.5 puntos) Realiza también un experimento con el mismo conjunto de datos, pero esta vez empleando un Random Forest\n",
    "(https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#random-forest-classifier, https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.tree.RandomForestModel) como modelo a entrenar. Para este modelo probaremos las siguientes horquillas de parámetros:\n",
    "\n",
    "    a. numTrees, probando 3 valores entre 1 y 100.\n",
    "    b. maxDepth, probando 3 valores entre 1 y 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución (hh:mm:ss.ms): 0:09:14.806680\n",
      "0.9282494687547282 50 10 2\n",
      "0.9303454709719057 50 50 2\n",
      "0.930875238895212 50 100 2\n",
      "0.926577295885161 100 10 2\n",
      "0.9317668014799373 100 50 2\n",
      "0.9304626829208582 100 100 2\n",
      "0.9373904520924319 50 10 5\n",
      "0.9380006946607123 50 50 5\n",
      "0.9376964903878597 50 100 5\n",
      "0.9378096745968093 100 10 5\n",
      "0.9361189228745124 100 50 5\n",
      "0.9368649004331362 100 100 5\n",
      "0.9471446523426482 50 10 9\n",
      "0.9487313043634578 50 50 9\n",
      "0.950034172164556 50 100 9\n",
      "0.9444828723759648 100 10 9\n",
      "0.9450860572112703 100 50 9\n",
      "0.9458679025080009 100 100 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9513280041258381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_inicial_rfc = dt.now()\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "ramdomForest = RandomForestClassifier(labelCol='winner_b',featuresCol='red_features')\n",
    "\n",
    "params_rfc = ParamGridBuilder().addGrid(ramdomForest.maxDepth, [2, 5, 9]).addGrid(pca.k, [50, 100]).addGrid(ramdomForest.numTrees, [10, 50, 100]).build()\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, pca, ramdomForest]) \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='winner_b')\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params_rfc).setNumFolds(5)\n",
    "model_rfc = cv.fit(train_df)\n",
    "\n",
    "t_final_rfc = dt.now()\n",
    "t_ejecucion_rfc = t_final_rfc - t_inicial_rfc\n",
    "print(f\"Tiempo de ejecución (hh:mm:ss.ms): {t_ejecucion_rfc}\")\n",
    "for (result, config) in zip(model_rfc.avgMetrics, params_rfc) :\n",
    "    print(result, config[pca.k], config[ramdomForest.numTrees], config[ramdomForest.maxDepth])\n",
    "\n",
    "ev_rfc = evaluator.evaluate(model_rfc.transform(test_df))\n",
    "ev_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (3 puntos) Prueba ahora sobre el mismo conjunto de datos, pero esta vez utilizando un modelo Gradient-boosted tree (https://spark.apache.org/docs/2.2.0/mlclassificationregression.html#gradient-boosted-tree-classifier, https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.tree.GradientBoostedTreesModel) como algoritmo a entrenar y eliminando la reducción de variables de PCA. Para este modelo probaremos las siguientes horquillas de parámetros:\n",
    "\n",
    "    a. maxIter, probando 2 valores entre 10 y 100.\n",
    "    \n",
    "    b. maxDepth, probando 2 valores entre 1 y 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución (hh:mm:ss.ms): 0:22:02.053396\n",
      "0.9546631187069472 30 2\n",
      "0.9668629164122036 30 8\n",
      "0.9615503589697858 75 2\n",
      "0.9671941207402944 75 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9663486333161423"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_inicial_gbt = dt.now()\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "GradientBoostedTree = GBTClassifier(labelCol='winner_b',featuresCol='features')\n",
    "\n",
    "params_gbt = ParamGridBuilder().addGrid(GradientBoostedTree.maxIter, [30, 75]).addGrid(GradientBoostedTree.maxDepth, [2, 8]).build()\n",
    "pipeline = Pipeline().setStages([cv1, cv2, binarizer, ohe, assembler1, scaler, assembler2, GradientBoostedTree]) \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='winner_b')\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params_gbt).setNumFolds(5)\n",
    "model_gbt = cv.fit(train_df)\n",
    "\n",
    "t_final_gbt = dt.now()\n",
    "t_ejecucion_gbt = t_final_gbt - t_inicial_gbt\n",
    "print(f\"Tiempo de ejecución (hh:mm:ss.ms): {t_ejecucion_gbt}\")\n",
    "for (result, config) in zip(model_gbt.avgMetrics, params_gbt) :\n",
    "    print(result, config[GradientBoostedTree.maxIter], config[GradientBoostedTree.maxDepth])\n",
    "\n",
    "ev_gbt = evaluator.evaluate(model_gbt.transform(test_df))\n",
    "ev_gbt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.5 punto) Compara los resultados obtenidos para los experimentos que hayas realizado. ¿Qué configuración ha sido la más exitosa? Calcula el tiempo medio por número de modelos entrenados respecto del tiempo total, ¿cuál ha tardado más? ¿Merece la mejora en los resultados frente al tiempo empleado? Genera una celda de texto incluyendo un párrafo con una conclusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo 'model_logistica' tiene una precisión del 0.9617070654976793%.\n",
      "Se han entrenado 8 modelos.\n",
      "Ha tardado (hh:mm:ss.ms)0:02:26.847367\n",
      "El tiempo medio por modelo es 0:00:18.355921\n",
      "El mejor modelo se ha ajustado un 0.9604198923050231%\n",
      "Los mejores parámetros son:\n",
      "elasticNetParam: 0.33\n",
      "k: 100\n",
      "regParam: 0.33\n",
      "maxIter: 50\n",
      "\n",
      "\n",
      "El modelo 'model_logistica2' tiene una precisión del 0.9617070654976793%.\n",
      "Se han entrenado 96 modelos.\n",
      "Ha tardado (hh:mm:ss.ms)0:21:42.198405\n",
      "El tiempo medio por modelo es 0:00:13.564567\n",
      "El mejor modelo se ha ajustado un 0.9604198923050231%\n",
      "Los mejores parámetros son:\n",
      "elasticNetParam: 0.75\n",
      "k: 100\n",
      "regParam: 5.0\n",
      "maxIter: 80\n",
      "\n",
      "\n",
      "El modelo 'model_rfc' tiene una precisión del 0.9513280041258381%.\n",
      "Se han entrenado 18 modelos.\n",
      "Ha tardado (hh:mm:ss.ms)0:09:14.806680\n",
      "El tiempo medio por modelo es 0:00:30.822593\n",
      "El mejor modelo se ha ajustado un 0.950034172164556%\n",
      "Los mejores parámetros son:\n",
      "maxDepth: 9\n",
      "k: 100\n",
      "numTrees: 100\n",
      "\n",
      "\n",
      "El modelo 'model_gbt' tiene una precisión del 0.9663486333161423%.\n",
      "Se han entrenado 4 modelos.\n",
      "Ha tardado (hh:mm:ss.ms)0:22:02.053396\n",
      "El tiempo medio por modelo es 0:05:30.513349\n",
      "El mejor modelo se ha ajustado un 0.9671941207402944%\n",
      "Los mejores parámetros son:\n",
      "maxIter: 75\n",
      "maxDepth: 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelos = [\"logistica\", \"logistica2\", \"rfc\", \"gbt\"]\n",
    "for mo in modelos:\n",
    "    n_model = \"model_\"+mo # Nombre del modelo\n",
    "    #print (n_model)\n",
    "    modelo = eval(n_model)\n",
    "    ev_model = \"ev_\"+mo\n",
    "    evaluador = eval(ev_model)\n",
    "    #print(evaluador)\n",
    "    params_model = \"params_\"+mo\n",
    "    parametros = eval(params_model)\n",
    "    metricas_model = modelo.avgMetrics\n",
    "    cantidad_metricas_model = len(metricas_model)\n",
    "    #print(metricas_model)\n",
    "    #print(cantidad_metricas_model)\n",
    "    t_inicial = \"t_inicial_\"+mo\n",
    "    t_inicial_model = eval(t_inicial)\n",
    "    t_final = \"t_final_\"+mo\n",
    "    t_final_model = eval(t_final)\n",
    "    t_diferencia = t_final_model - t_inicial_model\n",
    "    tiempo_por_modelo = t_diferencia / cantidad_metricas_model\n",
    "    mejor_modelo = max(metricas_model)\n",
    "    mejores_parametros = \"\"\n",
    "    for (result, config) in zip(metricas_model, parametros) :\n",
    "        if(result == mejor_modelo):\n",
    "            mejores_parametros = config\n",
    "    \n",
    "\n",
    "    print(\"El modelo '\", n_model, \"' tiene una precisión del \",evaluador,\"%.\",\\\n",
    "          \"\\nSe han entrenado \", cantidad_metricas_model, \" modelos.\"\\\n",
    "          \"\\nHa tardado (hh:mm:ss.ms)\",t_diferencia,\\\n",
    "          \"\\nEl tiempo medio por modelo es \", tiempo_por_modelo,\\\n",
    "          \"\\nEl mejor modelo se ha ajustado un \",mejor_modelo, \"%\",\\\n",
    "          sep='')\n",
    "    print(\"Los mejores parámetros son:\")\n",
    "    for (key,value) in zip(config.keys(), config.values()):\n",
    "        print(key.name,\": \", value, sep=\"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración más exitosa ha sido la última; que utiliza un clasificador \"Gradient-boosted tree\". Ha logrado una precisión del 96,72% en los datos de test. Sin embargo, también ha sido la que más ha tardado. En entrenar 4 modelos ha empleado más de 20 minutos. Esto es, a más de 5:30 minutos por modelo, y la precisión obtenida no difiere mucho de las demás configuraciones. La peor, logra 95% en poco más de 30 segundos por modelo (tercer modelo). La más rápida, que utiliza regresión logística, logra una precisión del 96,04% en los datos de test, empleando menos de 14 segundos por modelo (segundo modelo). Curiosamente, esta es la misma precisión que se logra con esta misma clase de regresión y disminuyendo el número de parámetros a ajustar (primer modelo). El segundo modelo tarda más tiempo porque combinan más parámetros, de forma que el tiempo total es excesivo. Curiosamente, la configuración de parámetros elegida no es la misma que en el primer modelo, pero obtiene la misma precisión con los datos de test.\n",
    "\n",
    "En conclusión. Teniendo en cuenta los resultados, el mejor modelo es el primero, que utiliza regresión logística y pocos parámetros, porque logra una precisión muy alta, en un tiempo total muy bajo, menor a 2:30 minutos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
